import os
import sys

import fire
import gradio as gr
import torch
import transformers
from peft import PeftModel

from transformers import pipeline, BitsAndBytesConfig

# í›ˆë ¨ê°€ì¤‘ì¹˜ ìµœì¢… ëˆ„ì  ê²½ë¡œë¥¼ ì–»ê¸° ìœ„í•´
from transformers.trainer_utils import get_last_checkpoint

from utils import Iteratorize
from utils import Prompter


if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

try:
    if torch.backends.mps.is_available():
        device = "mps"
except:  # noqa: E722
    pass


def main(
    base_model: str = "",
    peft_root: str = None,
    cache_dir: str = None,
    load_4bit: bool = False,
    load_8bit: bool = False,
    prompt_template: str = "polyglot-ko",  
    server_name: str = "0.0.0.0",
    share_gradio: bool = True,
    use_pipe: bool = False
):
    # --------------------------------------
    # ì´ˆê¸° ì‘ì—…
    # --------------------------------------
    GPUs = torch.cuda.device_count()
    # ê¸°ë°˜ëª¨ë¸ í™•ì¸
    base_model = base_model or os.environ.get("BASE_MODEL", "")
    assert(base_model), "Please specify a --base_model, e.g. --base_model='resources/polyglot-ko-1.3b'"
    assert(os.path.isdir(base_model)), f"Found not a path for base_model: {base_model}"
    # í”„ë¡¬í”„í„° ìƒì„±
    prompter = Prompter(prompt_template)
    # íŒŒë¼ë¯¸í„° ì²´í¬
    if load_4bit == load_8bit:
        raise ValueError(f"Please specify either 4bit({load_4bit}) or 8bit({load_8bit}) model.")
    # --------------------------------------
    # ì •ë³´ í™”ë©´ì¶œë ¥
    # --------------------------------------
    print("----------------------------------------")
    print(f"platform: {sys.platform}, torch: {torch.__version__}, GPU: {GPUs}, device: {device}")
    print("----------------------------------------")
    # [LOCAL_RANK] í™˜ê²½ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜, [0]ì´ë©´ íŒŒë¼ë¯¸í„° ì¶œë ¥
    if int(os.environ.get("LOCAL_RANK", 0)) == 0:
        print(
            f"Generate params:\n"
            f"base_model: {base_model}\n"
            f"peft_root: {peft_root}\n"
            f"cache_dir: {cache_dir}\n"
            f"load_4bit: {load_4bit}\n"
            f"load_8bit: {load_8bit}\n"
            f"prompt_template: {prompt_template}\n"
            f"server_name: {server_name}\n"
            f"share_gradio: {share_gradio}"
        )
        print("----------------------------------------")
    # --------------------------------------
    # 0. ì¤€ë¹„ ì‘ì—…
    # --------------------------------------
    # 0-1. PEFT ë£¨íŠ¸ê²½ë¡œ(peft_root)ë¡œë¶€í„°, ìµœì¢… ì²´í¬í¬ì¸íŠ¸(í›ˆë ¨ê°€ì¤‘ì¹˜) ê²½ë¡œ(last_peft_path) ì–»ê¸° : ì—†ìœ¼ë©´ ì‚¬ìš©ì•ˆí•¨
    # ------------------
    last_peft_path = None
    if peft_root:
        last_peft_path = get_last_checkpoint(peft_root)
        if last_peft_path is None:
            if len(os.listdir(peft_root)) > 0:
                raise ValueError(f"PEFT directory({peft_root}) already exists and is not empty.")
            else:
                raise ValueError(f"Found not checkpoint for PEFT in {peft_root}.")
        else:
            print(
                f"Checkpoint detected: {last_peft_path}\n"
                "----------------------------------------"
            )
    # ------------------
    # 0-2. í† í¬ë‚˜ì´ì €(tokenizer) ë¡œë“œ
    # ------------------
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        base_model,
        device_map={"": device},
        cache_dir=cache_dir,
        padding_side="right",
        use_fast=False,
    )
    # --------------------------------------
    # 1. device íƒ€ì…ì— ë”°ë¥¸, ì‚¬ì „í›ˆë ¨ëª¨ë¸(komodel) & PEFT ê°€ì¤‘ì¹˜ ë¡œë“œ
    # --------------------------------------
    # 1-1. cuda
    # ------------------
    if device == "cuda":
        komodel = transformers.AutoModelForCausalLM.from_pretrained(
            base_model,
            #load_in_8bit=load_8bit,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=load_4bit,
                load_in_8bit=load_8bit,
                load_in_8bit_fp32_cpu_offload=load_8bit,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=load_8bit,
                bnb_4bit_quant_type='nf4'
            ),
            torch_dtype=torch.float16,
            device_map={"": device},
            cache_dir=cache_dir,
        )
        if last_peft_path:
            komodel = PeftModel.from_pretrained(
                komodel,
                last_peft_path,
                torch_dtype=torch.float16,
            )
    # ------------------
    # 1-2. mps
    # ------------------
    elif device == "mps":
        komodel = transformers.AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map={"": device},
            cache_dir=cache_dir,
        )
        if last_peft_path:
            komodel = PeftModel.from_pretrained(
                komodel,
                last_peft_path,
                device_map={"": device},
                torch_dtype=torch.float16
            )
    # ------------------
    # 1-3. cpu
    # ------------------
    else:
        komodel = transformers.AutoModelForCausalLM.from_pretrained(
            base_model,
            device_map={"": device},
            low_cpu_mem_usage=True,
            cache_dir=cache_dir,
        )
        if last_peft_path:
            komodel = PeftModel.from_pretrained(
                komodel,
                last_peft_path,
                device_map={"": device},
                torch_dtype=torch.float16,
            )
    # --------------------------------------
    #
    # --------------------------------------
    """
    push_to_hub()ëŠ” ìì‹ ì´ í›ˆë ¨ì‹œí‚¨ ëª¨ë¸ì„ HuggingFaceì— í‘¸ì‹œí•˜ëŠ” í•¨ìˆ˜ë‹¤.
    ì•„ë˜ì˜ push_to_hub()í•¨ìˆ˜ê°€ ì‘ë™í•˜ë ¤ë©´, ìì‹ ì˜ HuggingFace ê³„ì •ì—, repo_idë¥¼ êµ¬ì¶•í•˜ê³ , push_to_hub()í•¨ìˆ˜ì— ì œê³µí•´ì•¼ í•œë‹¤.
        1. ì‚¬ìš©ì ê³µê°„ì— ê¸°ë³¸ì ìœ¼ë¡œ í•  ë•Œ,
            tokenizer.push_to_hub("my-tokenizer")
        2. ì‚¬ìš©ì ëª…ì¹­ ëª…ì‹œí•  ë•Œ,
            tokenizer.push_to_hub("username/my-tokenizer")
        3. ì¡°ì§ ëª…ì¹­ì„ ëª…ì‹œí•  ë•Œ
            tokenizer.push_to_hub("my-organization/my-tokenizer")
    ì´ë˜ë„ ì•ˆëœë‹¤ë©´, ìì‹ ì´ repo_idë¥¼ êµ¬ì¶•í•˜ì§€ ì•Šì€ ê²ƒì…ë‹ˆë‹¤.
    """
    #komodel.push_to_hub('EleutherAI/polyglot-ko-1.3b')
    # --------------------------------------
    # ì–‘ìí™” ëª¨ë¸ì€ half()ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    if not load_8bit:
        komodel.half()
    """
    # --------------------------------------
    # ì‚¬ì „í›ˆë ¨ëª¨ë¸ í‰ê°€ ì§„í–‰
    # --------------------------------------
    komodel.eval()
    # --------------------------------------
    # ì‚¬ì „í›ˆë ¨ëª¨ë¸ ì»´íŒŒì¼
    # --------------------------------------
    if torch.__version__ >= "2" and sys.platform != "win32":
        #  PyTorch ì½”ë“œë¥¼ JIT ì»´íŒŒì¼í•˜ì—¬, ìµœì í™”ëœ ì»¤ë„ë¡œ ì‹¤í–‰
        komodel = torch.compile(komodel)
    # --------------------------------------
    # í”„ë¡¬í”„íŠ¸ ë„ìš°ê¸°
    # --------------------------------------
    pipe = None
    examples = [
        ["ì–‘íŒŒëŠ” ì‹ë¬¼ì˜ ì–´ë–¤ ë¶€ìœ„ì¸ê°€ìš”? ê·¸ë¦¬ê³  ê³ êµ¬ë§ˆëŠ” ë¿Œë¦¬ì¸ê°€ìš”?"],
        ["ìŠ¤ì›¨í„°ì˜ ìœ ë˜ëŠ” ì–´ë””ì—ì„œ ì‹œì‘ë˜ì—ˆë‚˜ìš”?"],
        ["í† ì„±ì˜ ê³ ë¦¬ê°€ ë¹›ì˜ ë ë¡œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"]
    ]
    bad_words = []
    if use_pipe:
        pipe = pipeline(
            "text-generation",
            model=komodel,
            tokenizer=tokenizer,
            eos_token_id=tokenizer.eos_token_id
        )
    else:
        bad_words = [
            '....',
            'http://'
        ]
    # ì¶”ë¡ (ì§ˆì˜ > ì‘ë‹µ)
    def inference(
        instruction,
        input=None,
        temperature=0.8,
        top_p=0.8,
        top_k=50,
        max_new_tokens=64,
        **kwargs,
    ):
        prompt = prompter.generate_ask(instruction, input)
        print(f"prompt: {prompt}")
        # í›ˆë ¨ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ : ì—¬ëŸ¬ ë¬¸ì¥ì„ í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
        ask_msg = [{"role": "ì§ˆë¬¸", "content": prompt}]
        question = "\n".join([f"### {msg['role']}: {msg['content']}" for msg in ask_msg])
        question += "\n\n### ë‹µë³€:"
        print(f"question: {question}")
        # ì‚¬ì „í›ˆë ¨ëª¨ë¸ì— ì§ˆì˜/ì‘ë‹µ(ìƒì„±)
        if use_pipe and pipe:
            answer = pipe(
                question,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_new_tokens=max_new_tokens,
                #return_full_text=False,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id
            )[0]['generated_text']
        else:
            # [tokenizer/komodel]ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬, ì§ˆë‹µ ì²˜ë¦¬
            with torch.no_grad():
                # í† í°í™”
                ask_tokens = tokenizer(question, return_tensors="pt").input_ids.to(device)
                # ìƒì„±í† ë¡ í•˜ê³ , question ë‚´ì— <|endoftext|>ê°€ ì—†ë‹¤ë©´, ìƒì„±ì„ ì¢…ë£Œí•œë‹¤.
                gen_tokens = komodel.generate(
                    ask_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_new_tokens=max_new_tokens,
                    no_repeat_ngram_size=3,
                    repetition_penalty=1.2,
                    bad_words_ids=[
                        tokenizer.encode(bad_word) for bad_word in bad_words
                    ],
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id
                )
                # í† í°ì„ ë¬¸ì¥í™”
                answer = tokenizer.batch_decode(gen_tokens)[0]
        # ------------------
        # ì‘ë‹µë©”ì„¸ì§€ ë³´ì •
        # ------------------
        print(f"answer: {answer}")
        # ë‹µë³€ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• 
        answer = answer.split("### ")
        has_ans = False
        # ë‹µë³€ë§Œ ì¶”ì¶œ
        for one in answer:
            fidx = one.find("ë‹µë³€: ")
            if fidx >= 0 and fidx < 2:
                answer = one.lstrip("ë‹µë³€: ")
                has_ans = True
                break
        # ë‹µë³€ì´ ì—†ìœ¼ë©´
        if not has_ans:
            answer = "..."
        # íŠ¹ìˆ˜ í† í° ì œê±°
        answer = answer.rstrip(tokenizer.special_tokens_map['eos_token'])
        # ------------------
        return answer

    gr.Interface(
        fn=inference,
        inputs=[
            gr.components.Textbox(lines=2, label="Instruction", placeholder="Tell me what."),
            gr.components.Textbox(lines=2, label="Input", placeholder="none"),
            gr.components.Slider(minimum=0, maximum=1, value=0.8, label="Temperature"),
            gr.components.Slider(minimum=0, maximum=1, value=0.8, label="Top p"),
            gr.components.Slider(minimum=0, maximum=100, step=1, value=50, label="Top k"),
            gr.components.Slider(minimum=1, maximum=2000, step=1, value=256, label="Max tokens")
        ],
        outputs=[
            gr.Textbox(lines=5, label="Output")
        ],
        title="ğŸ¤— [polyglot-ko-1.3b-lite] ê°€ë²¼ìš´ í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ ë¯¸ì„¸ì¡°ì •(QLoRA) ğŸ¤—",
        examples=examples,
        description="""
<p align="center" width="100%">
<img src="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jpn8a_aJ5etAJwFUd_nno.png?w=200&h=200&f=face" alt="Joonkyu icon" style="width: 200px; display: block; margin: auto; border-radius: 20%;">
í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ í”„ë¡¬í”„íŠ¸
</p><br>

""",
        thumbnail="https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/jpn8a_aJ5etAJwFUd_nno.png?w=200&h=200&f=face",        
    ).queue().launch(server_name="0.0.0.0", share=share_gradio)
    # --------------------------------------
    
# --------------------------------------------------------------------------------------------------------------------------------------------------------------
if __name__ == "__main__":
    fire.Fire(main)